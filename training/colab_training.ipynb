{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2be9dba",
   "metadata": {},
   "source": [
    "# ðŸš€ Digital Sahayak AI - Training Notebook\n",
    "## Fine-tune your own custom AI model\n",
    "\n",
    "This notebook will help you train your own AI model for Digital Sahayak.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU (T4 free tier works, A100 recommended)\n",
    "- Your training data in JSON format\n",
    "- ~15GB disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49665b6b",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl sentencepiece\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fe035",
   "metadata": {},
   "source": [
    "## Step 2: Login to Hugging Face (for downloading base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "# Make sure you have accepted Mistral model terms\n",
    "HF_TOKEN = \"hf_your_token_here\"  # <-- Replace with your token\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"âœ… Logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b7414",
   "metadata": {},
   "source": [
    "## Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5075dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Change these as per your needs\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    # Your custom model name\n",
    "    \"output_model_name\": \"Pratham-ML\",\n",
    "    \n",
    "    # Training data path (upload your JSON file)\n",
    "    \"training_data\": \"train_data.json\",\n",
    "    \n",
    "    # Training parameters\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 2,\n",
    "    \"gradient_accumulation\": 8,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"max_seq_length\": 2048,\n",
    "    \n",
    "    # LoRA parameters\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Output directory\n",
    "    \"output_dir\": \"./output\",\n",
    "}\n",
    "\n",
    "print(f\"ðŸŽ¯ Training: {MODEL_CONFIG['output_model_name']}\")\n",
    "print(f\"ðŸ“Š Epochs: {MODEL_CONFIG['num_epochs']}\")\n",
    "print(f\"ðŸ“ Output: {MODEL_CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2793dc",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data\n",
    "\n",
    "Your training data should be in this format:\n",
    "```json\n",
    "[\n",
    "  {\"instruction\": \"PM-KISAN yojana kya hai?\", \"response\": \"PM-KISAN ek sarkari yojana hai...\"},\n",
    "  {\"instruction\": \"SSC MTS ki eligibility batao\", \"response\": \"SSC MTS ke liye 10th pass hona chahiye...\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea38fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Sample training data (replace with your own)\n",
    "SAMPLE_DATA = [\n",
    "    {\n",
    "        \"instruction\": \"PM-KISAN yojana kya hai?\",\n",
    "        \"response\": \"PM-KISAN (Pradhan Mantri Kisan Samman Nidhi) ek sarkari yojana hai jisme kisano ko har saal â‚¹6,000 ki madad milti hai. Yeh rashi 3 kisht mein â‚¹2,000-â‚¹2,000 karke di jaati hai. Is yojana ki shuruat 2018 mein hui thi.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"SSC MTS ki eligibility kya hai?\",\n",
    "        \"response\": \"SSC MTS (Multi Tasking Staff) ke liye eligibility:\\n- Education: 10th Pass (Matriculation)\\n- Age: 18-25 years\\n- Age relaxation: OBC - 3 years, SC/ST - 5 years\\n\\nWebsite: ssc.nic.in\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Bihar ka capital kya hai?\",\n",
    "        \"response\": \"Bihar ki rajdhani Patna hai. Patna ek bahut purana sheher hai aur Ganga nadi ke kinare basaa hai.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Aadhar card kaise banwaye?\",\n",
    "        \"response\": \"Aadhar card banwane ke liye:\\n1. Apne najdiki Aadhar Enrollment Center jaye\\n2. Required documents le jaaye (ID proof, address proof, DOB proof)\\n3. Biometric (fingerprint, iris scan) dein\\n4. Form bhare\\n5. 90 din mein Aadhar post se aa jayega\\n\\nOnline appointment: uidai.gov.in\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Good morning\",\n",
    "        \"response\": \"ðŸŒ… Suprabhat! Aapka din shubh ho! Main Digital Sahayak AI hoon. Aaj kisme madad karoon? Sarkari yojana, naukri, ya documents - kuch bhi poochiye!\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Ayushman Bharat yojana kab shuru hui?\",\n",
    "        \"response\": \"Ayushman Bharat yojana 23 September 2018 ko shuru hui thi. Ise PM Narendra Modi ne Jharkhand ke Ranchi se launch kiya tha. Is yojana mein garib parivaron ko â‚¹5 lakh tak ka free health insurance milta hai.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save sample data\n",
    "with open(MODEL_CONFIG['training_data'], 'w', encoding='utf-8') as f:\n",
    "    json.dump(SAMPLE_DATA, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Sample training data saved: {len(SAMPLE_DATA)} examples\")\n",
    "print(\"\\nâš ï¸ Upload your own 'train_data.json' with more examples for better results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd932879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "import json\n",
    "\n",
    "with open(MODEL_CONFIG['training_data'], 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Format for training\n",
    "def format_prompt(example):\n",
    "    return f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['response']}\"\"\"\n",
    "\n",
    "formatted_data = [{\"text\": format_prompt(d)} for d in raw_data]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(f\"âœ… Dataset loaded: {len(dataset)} examples\")\n",
    "print(f\"\\nðŸ“ Sample:\\n{dataset[0]['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354eaef",
   "metadata": {},
   "source": [
    "## Step 5: Load Base Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc30ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Base model (Apache 2.0 licensed)\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# 4-bit quantization config (saves memory)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"ðŸ“¥ Loading base model (this takes 2-5 minutes)...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"âœ… Base model loaded successfully!\")\n",
    "print(f\"ðŸ“Š Model size: ~{model.num_parameters()/1e9:.1f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06536c",
   "metadata": {},
   "source": [
    "## Step 6: Apply LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7487412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=MODEL_CONFIG['lora_r'],\n",
    "    lora_alpha=MODEL_CONFIG['lora_alpha'],\n",
    "    lora_dropout=MODEL_CONFIG['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"âœ… LoRA adapters applied!\")\n",
    "print(f\"ðŸ“Š Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "print(f\"ðŸ“Š Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1bb1c",
   "metadata": {},
   "source": [
    "## Step 7: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bdae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_CONFIG['output_dir'],\n",
    "    num_train_epochs=MODEL_CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=MODEL_CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=MODEL_CONFIG['gradient_accumulation'],\n",
    "    learning_rate=MODEL_CONFIG['learning_rate'],\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MODEL_CONFIG['max_seq_length'],\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "print(f\"ðŸ“Š Total steps: {len(trainer.get_train_dataloader()) * MODEL_CONFIG['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5989e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282dd611",
   "metadata": {},
   "source": [
    "## Step 8: Save LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapters\n",
    "adapter_path = f\"{MODEL_CONFIG['output_dir']}/lora_adapters\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"âœ… LoRA adapters saved to: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc6743",
   "metadata": {},
   "source": [
    "## Step 9: Merge Adapters with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2902511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"ðŸ”„ Merging adapters with base model...\")\n",
    "\n",
    "# Load base model in full precision for merging\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load and merge LoRA adapters\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "print(\"âœ… Adapters merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f157d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final merged model\n",
    "final_model_path = f\"{MODEL_CONFIG['output_dir']}/{MODEL_CONFIG['output_model_name']}\"\n",
    "\n",
    "merged_model.save_pretrained(final_model_path, safe_serialization=True)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"âœ… Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b2c78",
   "metadata": {},
   "source": [
    "## Step 10: Create LICENSE and Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LICENSE file (Apache 2.0 requirement)\n",
    "LICENSE_TEXT = \"\"\"Apache License\n",
    "Version 2.0, January 2004\n",
    "http://www.apache.org/licenses/\n",
    "\n",
    "Copyright 2023 Mistral AI\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{final_model_path}/LICENSE\", 'w') as f:\n",
    "    f.write(LICENSE_TEXT)\n",
    "\n",
    "print(\"âœ… LICENSE file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Card (README.md)\n",
    "MODEL_CARD = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "  - en\n",
    "  - hi\n",
    "tags:\n",
    "  - text-generation\n",
    "  - conversational\n",
    "  - hindi\n",
    "  - government-services\n",
    "---\n",
    "\n",
    "# {MODEL_CONFIG['output_model_name']}\n",
    "\n",
    "A conversational AI model for Indian government services assistance.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Model Name:** {MODEL_CONFIG['output_model_name']}\n",
    "- **Languages:** Hindi, English\n",
    "- **Use Case:** Government schemes, jobs, documents assistance\n",
    "- **License:** Apache 2.0\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{MODEL_CONFIG['output_model_name']}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{MODEL_CONFIG['output_model_name']}\")\n",
    "\n",
    "prompt = \"PM-KISAN yojana kya hai?\"\n",
    "inputs = tokenizer(f\"### Instruction:\\\\n{{prompt}}\\\\n\\\\n### Response:\\\\n\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## Training\n",
    "\n",
    "This model was trained using LoRA (Low-Rank Adaptation) technique.\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "This model is for informational purposes. Always verify information from official government sources.\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{final_model_path}/README.md\", 'w') as f:\n",
    "    f.write(MODEL_CARD)\n",
    "\n",
    "print(\"âœ… Model card (README.md) created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d09c53",
   "metadata": {},
   "source": [
    "## Step 11: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "def generate_response(prompt, max_tokens=256):\n",
    "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "    \n",
    "    outputs = merged_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Good morning\",\n",
    "    \"PM-KISAN yojana kya hai?\",\n",
    "    \"SSC MTS ki eligibility batao\",\n",
    "    \"Bihar ka capital kya hai?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing the model:\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"â“ User: {query}\")\n",
    "    response = generate_response(query)\n",
    "    print(f\"ðŸ¤– AI: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af5604",
   "metadata": {},
   "source": [
    "## Step 12: Download the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2979430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model for download\n",
    "import shutil\n",
    "\n",
    "zip_path = f\"{MODEL_CONFIG['output_model_name']}.zip\"\n",
    "shutil.make_archive(MODEL_CONFIG['output_model_name'], 'zip', final_model_path)\n",
    "\n",
    "print(f\"âœ… Model zipped: {zip_path}\")\n",
    "print(f\"ðŸ“¥ Download from Colab file browser or use:\")\n",
    "print(f\"   from google.colab import files\")\n",
    "print(f\"   files.download('{zip_path}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to Hugging Face Hub\n",
    "# Uncomment if you want to upload\n",
    "\n",
    "# from huggingface_hub import HfApi\n",
    "# \n",
    "# api = HfApi()\n",
    "# api.create_repo(repo_id=f\"your-username/{MODEL_CONFIG['output_model_name']}\", private=True)\n",
    "# api.upload_folder(\n",
    "#     folder_path=final_model_path,\n",
    "#     repo_id=f\"your-username/{MODEL_CONFIG['output_model_name']}\",\n",
    "# )\n",
    "# print(\"âœ… Model uploaded to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814fd89",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ‰ Done!\n",
    "\n",
    "Your custom **Pratham-ML** model is ready!\n",
    "\n",
    "### What you got:\n",
    "1. âœ… Fine-tuned model weights (`.safetensors`)\n",
    "2. âœ… Tokenizer files\n",
    "3. âœ… LICENSE file (Apache 2.0 compliance)\n",
    "4. âœ… Model card (README.md)\n",
    "\n",
    "### Next steps:\n",
    "1. Download the model zip file\n",
    "2. Extract and use in your application\n",
    "3. Or upload to Hugging Face Hub for easy access\n",
    "\n",
    "### Tips for better results:\n",
    "- Add more training data (500+ examples recommended)\n",
    "- Include diverse question types\n",
    "- Train for more epochs (3-5)\n",
    "- Use A100 GPU for faster training"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
