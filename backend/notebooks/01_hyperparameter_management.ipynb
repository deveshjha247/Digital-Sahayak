{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809be46a",
   "metadata": {},
   "source": [
    "# üîß Digital Sahayak - Hyperparameter Management\n",
    "\n",
    "This notebook demonstrates how to manage hyperparameters using **separate configuration files** instead of hardcoding values.\n",
    "\n",
    "## Benefits:\n",
    "- ‚úÖ Easy to modify without changing code\n",
    "- ‚úÖ Better experiment tracking\n",
    "- ‚úÖ Shareable configurations\n",
    "- ‚úÖ Version control friendly\n",
    "- ‚úÖ Type-safe with validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb4df6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efac4c",
   "metadata": {},
   "source": [
    "## 2. Define Hyperparameter Dataclasses\n",
    "\n",
    "Type-safe configuration with proper defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87096767",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training hyperparameters\"\"\"\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 500\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    \"\"\"Optimizer settings\"\"\"\n",
    "    type: str = \"adamw\"\n",
    "    betas: List[float] = field(default_factory=lambda: [0.9, 0.999])\n",
    "    eps: float = 1e-8\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model architecture\"\"\"\n",
    "    type: str = \"transformer\"\n",
    "    pretrained_model: str = \"ai4bharat/indic-bert\"\n",
    "    hidden_size: int = 768\n",
    "    num_labels: int = 15\n",
    "    dropout: float = 0.3\n",
    "\n",
    "@dataclass\n",
    "class IntentClassifierConfig:\n",
    "    \"\"\"Complete config for Intent Classifier\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    optimizer: OptimizerConfig = field(default_factory=OptimizerConfig)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "# Create default config\n",
    "default_config = IntentClassifierConfig()\n",
    "print(\"üìã Default Configuration:\")\n",
    "print(json.dumps(default_config.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf204e",
   "metadata": {},
   "source": [
    "## 3. Load Configuration from YAML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from our config file\n",
    "config_path = Path.cwd().parent / \"ai\" / \"config\" / \"hyperparameters.yaml\"\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    full_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded config from: {config_path}\")\n",
    "print(f\"\\nüìã Available models: {list(full_config.keys())}\")\n",
    "\n",
    "# Get intent classifier config\n",
    "intent_config = full_config.get('intent_classifier', {})\n",
    "print(f\"\\nüß† Intent Classifier Hyperparameters:\")\n",
    "print(f\"  Learning Rate: {intent_config['training']['learning_rate']}\")\n",
    "print(f\"  Batch Size: {intent_config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {intent_config['training']['epochs']}\")\n",
    "print(f\"  Optimizer: {intent_config['optimizer']['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76b720",
   "metadata": {},
   "source": [
    "## 4. Validate Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d295df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_hyperparams(config: Dict) -> List[str]:\n",
    "    \"\"\"Validate hyperparameters and return list of errors\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Training validation\n",
    "    training = config.get('training', {})\n",
    "    \n",
    "    if training.get('learning_rate', 0) <= 0:\n",
    "        errors.append(\"learning_rate must be positive\")\n",
    "    if training.get('learning_rate', 0) > 0.1:\n",
    "        errors.append(\"learning_rate > 0.1 is too high\")\n",
    "        \n",
    "    if training.get('batch_size', 0) <= 0:\n",
    "        errors.append(\"batch_size must be positive\")\n",
    "    if training.get('batch_size', 0) > 512:\n",
    "        errors.append(\"batch_size > 512 may cause OOM\")\n",
    "        \n",
    "    if training.get('epochs', 0) <= 0:\n",
    "        errors.append(\"epochs must be positive\")\n",
    "    if training.get('epochs', 0) > 100:\n",
    "        errors.append(\"epochs > 100 may overfit\")\n",
    "    \n",
    "    # Model validation\n",
    "    model = config.get('model', {})\n",
    "    if model.get('dropout', 0) < 0 or model.get('dropout', 0) > 1:\n",
    "        errors.append(\"dropout must be between 0 and 1\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "# Validate our config\n",
    "errors = validate_hyperparams(intent_config)\n",
    "if errors:\n",
    "    print(\"‚ùå Validation Errors:\")\n",
    "    for e in errors:\n",
    "        print(f\"  - {e}\")\n",
    "else:\n",
    "    print(\"‚úÖ All hyperparameters are valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9feabed",
   "metadata": {},
   "source": [
    "## 5. Use ConfigManager from Our Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our config module\n",
    "from ai.config import config, get_hyperparams\n",
    "\n",
    "# Get hyperparameters with dot notation\n",
    "lr = config.get(\"intent_classifier.training.learning_rate\")\n",
    "batch_size = config.get(\"intent_classifier.training.batch_size\")\n",
    "epochs = config.get(\"intent_classifier.training.epochs\")\n",
    "\n",
    "print(\"üìä Using ConfigManager:\")\n",
    "print(f\"  Learning Rate: {lr}\")\n",
    "print(f\"  Batch Size: {batch_size}\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "\n",
    "# Get complete hyperparams as flat dict\n",
    "hyperparams = get_hyperparams(\"intent_classifier\")\n",
    "print(f\"\\nüìã Flattened Hyperparams: {list(hyperparams.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8bdb9",
   "metadata": {},
   "source": [
    "## 6. Example: Training Loop with Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619df577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name: str = \"intent_classifier\"):\n",
    "    \"\"\"\n",
    "    Example training function that uses config file for hyperparameters.\n",
    "    \n",
    "    Instead of hardcoding:\n",
    "        lr = 0.00002  # ‚ùå Bad practice\n",
    "        \n",
    "    We use:\n",
    "        lr = config.get(\"intent_classifier.training.learning_rate\")  # ‚úÖ Good practice\n",
    "    \"\"\"\n",
    "    # Load hyperparameters from config\n",
    "    hp = get_hyperparams(model_name)\n",
    "    \n",
    "    print(f\"üöÄ Training {model_name} with:\")\n",
    "    print(f\"  Epochs: {hp['epochs']}\")\n",
    "    print(f\"  Batch Size: {hp['batch_size']}\")\n",
    "    print(f\"  Learning Rate: {hp['learning_rate']}\")\n",
    "    print(f\"  Optimizer: {hp['optimizer_type']}\")\n",
    "    \n",
    "    # Simulated training loop\n",
    "    for epoch in range(1, min(hp['epochs'] + 1, 4)):  # Only 3 epochs for demo\n",
    "        train_loss = 1.0 / epoch  # Simulated\n",
    "        print(f\"  Epoch {epoch}: loss = {train_loss:.4f}\")\n",
    "    \n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    return {\"final_loss\": train_loss}\n",
    "\n",
    "# Run training\n",
    "result = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24b1c5",
   "metadata": {},
   "source": [
    "## 7. View All Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e514b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all model configs\n",
    "models = ['intent_classifier', 'field_classifier', 'job_recommender', 'content_rewriter']\n",
    "\n",
    "print(\"üìä Model Hyperparameter Comparison:\\n\")\n",
    "print(f\"{'Model':<20} {'Epochs':<10} {'Batch Size':<12} {'Learning Rate':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model in models:\n",
    "    cfg = config.get_training_config(model)\n",
    "    if cfg:\n",
    "        print(f\"{model:<20} {cfg.get('epochs', '-'):<10} {cfg.get('batch_size', '-'):<12} {cfg.get('learning_rate', '-'):<15}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
